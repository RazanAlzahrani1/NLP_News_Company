{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d0aa5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from textblob import TextBlob, Word\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn import datasets\n",
    "\n",
    "from sklearn.model_selection import  RandomizedSearchCV,GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "65ce5e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,precision_score, auc,recall_score, precision_recall_curve, fbeta_score, confusion_matrix, classification_report, make_scorer,f1_score,roc_auc_score\n",
    "\n",
    "def model_eval(mdl_local, X_local, y_local, bta =1):\n",
    "    y_pred = mdl_local.predict(X_local)\n",
    "    \n",
    "    accuracy= accuracy_score(y_local, y_pred)\n",
    "    f1= fbeta_score(y_local, y_pred, beta= bta, average='micro')\n",
    "    precision= precision_score(y_local, y_pred, average='micro')\n",
    "    recall= recall_score(y_local, y_pred, average='micro')\n",
    "    print(\"Accurcy :\",accuracy)\n",
    "    print(f\"F({bta}):\",f1)\n",
    "    print(\"Precision :\",precision)\n",
    "    print(\"Recall :\",recall)\n",
    "    \n",
    "    cm_results = [accuracy, precision, recall, f1]\n",
    "    return cm_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f53da96",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>clickbait</th>\n",
       "      <th>topices</th>\n",
       "      <th>popular_topices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>get bings</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tv female friend group belong</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>new star war force awakens trailer give chill</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vine new york celebrity big brother fucking pe...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>couple stunning photo shoot baby learning inop...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Lifestyle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15994</th>\n",
       "      <td>mini sisterhood traveling pant reunion</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15995</th>\n",
       "      <td>dog thankful best friend</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Lifestyle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15996</th>\n",
       "      <td>people proving dick big dropping condom head</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Lifestyle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15997</th>\n",
       "      <td>im atheist im</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15998</th>\n",
       "      <td>artist drew disney men justin bieber outcome g...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Entertainment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15999 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                headline  clickbait  topices  \\\n",
       "0                                              get bings          1        0   \n",
       "1                          tv female friend group belong          1        0   \n",
       "2          new star war force awakens trailer give chill          1        0   \n",
       "3      vine new york celebrity big brother fucking pe...          1        0   \n",
       "4      couple stunning photo shoot baby learning inop...          1        2   \n",
       "...                                                  ...        ...      ...   \n",
       "15994             mini sisterhood traveling pant reunion          1        1   \n",
       "15995                           dog thankful best friend          1        2   \n",
       "15996       people proving dick big dropping condom head          1        2   \n",
       "15997                                      im atheist im          1        0   \n",
       "15998  artist drew disney men justin bieber outcome g...          1        0   \n",
       "\n",
       "      popular_topices  \n",
       "0       Entertainment  \n",
       "1       Entertainment  \n",
       "2       Entertainment  \n",
       "3       Entertainment  \n",
       "4           Lifestyle  \n",
       "...               ...  \n",
       "15994        Politics  \n",
       "15995       Lifestyle  \n",
       "15996       Lifestyle  \n",
       "15997   Entertainment  \n",
       "15998   Entertainment  \n",
       "\n",
       "[15999 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('DataAfterMask.csv', index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e6d1717",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 15999 entries, 0 to 15998\n",
      "Data columns (total 4 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   headline         15997 non-null  object\n",
      " 1   clickbait        15999 non-null  int64 \n",
      " 2   topices          15999 non-null  int64 \n",
      " 3   popular_topices  15999 non-null  object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 625.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "556be1f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "headline           2\n",
       "clickbait          0\n",
       "topices            0\n",
       "popular_topices    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b99e9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3d2260",
   "metadata": {},
   "source": [
    "# determine lable and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56668aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df.headline\n",
    "y=df.topices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "644476a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b300c5",
   "metadata": {},
   "source": [
    "# count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2289d73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first document-term matrix has default Count Vectorizer values - counts of unigrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv1 = CountVectorizer(stop_words='english')\n",
    "\n",
    "X_train_cv1 = cv1.fit_transform(X_train)\n",
    "X_test_cv1  = cv1.transform(X_test)\n",
    "\n",
    "cv2 = CountVectorizer(ngram_range=(1,2), binary=True, stop_words='english')\n",
    "\n",
    "X_train_cv2 = cv2.fit_transform(X_train)\n",
    "X_test_cv2  = cv2.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cd7a33",
   "metadata": {},
   "source": [
    "### logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9a8a0b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Logistic Regression : 0.9669554344913817\n",
      "Accurcy : 1.0\n",
      "F(1): 1.0\n",
      "Precision : 1.0\n",
      "Recall : 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0, 1.0, 1.0, 1.0]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train_cv1, y_train)\n",
    "y_pred_cv1 = lr.predict(X_test_cv1)\n",
    "print(\"Train Logistic Regression :\",lr.score(X_train_cv1, y_train))\n",
    "me1=model_eval(lr,X_test_cv1,y_pred_cv1)\n",
    "me1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d8b57a15",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Logistic Regression : 0.9992855229079217\n",
      "Accurcy : 1.0\n",
      "F(1): 1.0\n",
      "Precision : 1.0\n",
      "Recall : 1.0\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train_cv2, y_train)\n",
    "y_pred_cv2 = lr.predict(X_test_cv2)\n",
    "print(\"Train Logistic Regression :\",lr.score(X_train_cv2, y_train))\n",
    "me2=model_eval(lr,X_test_cv2,y_pred_cv2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8ee207",
   "metadata": {},
   "source": [
    "### Naive Bayes-Multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2f498a7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurcy : 1.0\n",
      "F(1): 1.0\n",
      "Precision : 1.0\n",
      "Recall : 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train_cv1, y_train)\n",
    "y_pred_cv1_nb = mnb.predict(X_test_cv1)\n",
    "me3=model_eval(mnb,X_test_cv1,y_pred_cv1_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2e02b253",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurcy : 1.0\n",
      "F(1): 1.0\n",
      "Precision : 1.0\n",
      "Recall : 1.0\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train_cv2, y_train)\n",
    "y_pred_cv2_nb = mnb.predict(X_test_cv2)\n",
    "me4=model_eval(mnb,X_test_cv2,y_pred_cv2_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926bb57a",
   "metadata": {},
   "source": [
    "### Naive Bayes-Bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3aae32c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurcy : 1.0\n",
      "F(1): 1.0\n",
      "Precision : 1.0\n",
      "Recall : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Fit the second Naive Bayes model\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(X_train_cv1, y_train)\n",
    "y_pred_cv1_nb = bnb.predict(X_test_cv1)\n",
    "me5=model_eval(bnb,X_test_cv1,y_pred_cv1_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "110d1ceb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurcy : 1.0\n",
      "F(1): 1.0\n",
      "Precision : 1.0\n",
      "Recall : 1.0\n"
     ]
    }
   ],
   "source": [
    "bnb = BernoulliNB()\n",
    "bnb.fit(X_train_cv2, y_train)\n",
    "y_pred_cv2_nb = bnb.predict(X_test_cv2)\n",
    "me6=model_eval(bnb,X_test_cv2,y_pred_cv2_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6e60f9",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3c50366e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF-IDF versions of the Count Vectorizers created earlier in the exercise\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf1 = TfidfVectorizer(stop_words='english')\n",
    "X_train_tfidf1 = tfidf1.fit_transform(X_train)\n",
    "X_test_tfidf1  = tfidf1.transform(X_test)\n",
    "\n",
    "tfidf2 = TfidfVectorizer(ngram_range=(1,2), binary=True, stop_words='english')\n",
    "X_train_tfidf2 = tfidf2.fit_transform(X_train)\n",
    "X_test_tfidf2  = tfidf2.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcb0849",
   "metadata": {},
   "source": [
    "### logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a8ff9712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurcy : 1.0\n",
      "F(1): 1.0\n",
      "Precision : 1.0\n",
      "Recall : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Fit the first logistic regression on the TF-IDF data\n",
    "lr.fit(X_train_tfidf1, y_train)\n",
    "y_pred_tfidf1_lr = lr.predict(X_test_tfidf1)\n",
    "me7=model_eval(lr,X_test_tfidf1,y_pred_tfidf1_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "32791273",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurcy : 1.0\n",
      "F(1): 1.0\n",
      "Precision : 1.0\n",
      "Recall : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Fit the first logistic regression on the TF-IDF data\n",
    "lr.fit(X_train_tfidf2, y_train)\n",
    "y_pred_tfidf2_lr = lr.predict(X_test_tfidf2)\n",
    "me8=model_eval(lr,X_test_tfidf2,y_pred_tfidf2_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae5573e",
   "metadata": {},
   "source": [
    "### Naive Bayes-Multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "32b61c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurcy : 1.0\n",
      "F(1): 1.0\n",
      "Precision : 1.0\n",
      "Recall : 1.0\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train_tfidf1, y_train)\n",
    "y_pred_tfidf1_nb = mnb.predict(X_test_tfidf1)\n",
    "\n",
    "me9=model_eval(mnb,X_test_tfidf1,y_pred_tfidf1_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "29ea27eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurcy : 1.0\n",
      "F(1): 1.0\n",
      "Precision : 1.0\n",
      "Recall : 1.0\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train_tfidf2, y_train)\n",
    "y_pred_tfidf2_nb = mnb.predict(X_test_tfidf2)\n",
    "\n",
    "me10=model_eval(mnb,X_test_tfidf2,y_pred_tfidf2_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668ed660",
   "metadata": {},
   "source": [
    "### Naive Bayes-Bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6209ea02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurcy : 1.0\n",
      "F(1): 1.0\n",
      "Precision : 1.0\n",
      "Recall : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Fit the second Naive Bayes model\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(X_train_tfidf1, y_train)\n",
    "y_pred_tfidf_nb = bnb.predict(X_test_tfidf1)\n",
    "me11=model_eval(bnb,X_test_tfidf1,y_pred_tfidf_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ec59c398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurcy : 1.0\n",
      "F(1): 1.0\n",
      "Precision : 1.0\n",
      "Recall : 1.0\n"
     ]
    }
   ],
   "source": [
    "bnb = BernoulliNB()\n",
    "bnb.fit(X_train_tfidf2, y_train)\n",
    "y_pred_tfidf2_nb = bnb.predict(X_test_tfidf2)\n",
    "me12=model_eval(bnb,X_test_tfidf2,y_pred_tfidf2_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b03c7d8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LR1-CV</th>\n",
       "      <th>LR2-CV</th>\n",
       "      <th>NB1-MN-CV</th>\n",
       "      <th>NB2-MN-CV</th>\n",
       "      <th>NB1-BR-CV</th>\n",
       "      <th>NB2-BR-CV</th>\n",
       "      <th>LR1-TFIDF</th>\n",
       "      <th>LR2-TFIDF</th>\n",
       "      <th>NB1-MN-TFIDF</th>\n",
       "      <th>NB2-MN-TFIDF</th>\n",
       "      <th>NB1-BR-TFIDF</th>\n",
       "      <th>NB2-BR-TFIDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           LR1-CV  LR2-CV  NB1-MN-CV  NB2-MN-CV  NB1-BR-CV  NB2-BR-CV  \\\n",
       "Accuracy      1.0     1.0        1.0        1.0        1.0        1.0   \n",
       "Precision     1.0     1.0        1.0        1.0        1.0        1.0   \n",
       "Recall        1.0     1.0        1.0        1.0        1.0        1.0   \n",
       "F1 Score      1.0     1.0        1.0        1.0        1.0        1.0   \n",
       "\n",
       "           LR1-TFIDF  LR2-TFIDF  NB1-MN-TFIDF  NB2-MN-TFIDF  NB1-BR-TFIDF  \\\n",
       "Accuracy         1.0        1.0           1.0           1.0           1.0   \n",
       "Precision        1.0        1.0           1.0           1.0           1.0   \n",
       "Recall           1.0        1.0           1.0           1.0           1.0   \n",
       "F1 Score         1.0        1.0           1.0           1.0           1.0   \n",
       "\n",
       "           NB2-BR-TFIDF  \n",
       "Accuracy            1.0  \n",
       "Precision           1.0  \n",
       "Recall              1.0  \n",
       "F1 Score            1.0  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile all of the error metrics into a dataframe for comparison\n",
    "results_tf = pd.DataFrame(list(zip(me1, me2, me3, me4,me5, me6, me7, me8,me9, me10, me11, me12)))\n",
    "results_tf = results_tf.set_index([['Accuracy', 'Precision', 'Recall', 'F1 Score']])\n",
    "results_tf.columns = ['LR1-CV', 'LR2-CV', 'NB1-MN-CV', 'NB2-MN-CV','NB1-BR-CV', 'NB2-BR-CV','LR1-TFIDF', 'LR2-TFIDF', 'NB1-MN-TFIDF', 'NB2-MN-TFIDF','NB1-BR-TFIDF', 'NB2-BR-TFIDF']\n",
    "results_tf\n",
    "#'Accuracy', 'F1 Score', 'Recall', 'Precision'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8617af46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
